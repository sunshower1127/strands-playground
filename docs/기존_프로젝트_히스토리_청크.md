1. ì±„íŒ… íˆìŠ¤í† ë¦¬ í¬í•¨ ë°©ì‹
   íˆìŠ¤í† ë¦¬ ë¡œë“œ (S3ì—ì„œ)
   claude.pyLines 775-794 # ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬ API conversation_history: Dict[int, List[Message]] = {} conversation_data = await load_conversation_from_s3(current_user.id, conversation_id) if conversation_data: # S3ì— ê¸°ì¡´ ëŒ€í™”ê°€ ìˆìœ¼ë©´ ë©”ëª¨ë¦¬ì— ë¡œë“œ conversation_history[conversation_id] = [ Message(\*\*msg) for msg in conversation_data['messages'] ] # íˆìŠ¤í† ë¦¬ í† í° ê³„ì‚° message_list = [{'role': msg.role, 'content': msg.content} for msg in conversation_history[conversation_id]] print(f"ğŸ”„ íˆìŠ¤í† ë¦¬ í† í° ê³„ì‚° ì¤‘ ({len(message_list)}ê°œ ë©”ì‹œì§€)...") history_tokens = TokenCounter.count_messages_tokens(message_list) token_tracker.add_input_tokens(history_tokens, source='history') print(f"ğŸ“š ëŒ€í™” íˆìŠ¤í† ë¦¬ í† í°: {history_tokens:,}") else: # S3ì—ë„ ì—†ìœ¼ë©´ ìƒˆë¡œìš´ ëŒ€í™” ì‹œì‘ conversation_history[conversation_id] = []
   Flashë¡œ ë§¥ë½ ë¶„ì„ ë° ê²€ìƒ‰ì–´ ê°œì„ 
   gemini.pyLines 361-410
   async def analyze_context_with_flash(current_msg: str, conversation_history: list) -> str: """Gemini Flashë¥¼ ì‚¬ìš©í•˜ì—¬ ë§¥ë½ ë¶„ì„ ë° ê²€ìƒ‰ì–´ ê°œì„  (ìµœì‹  SDK ë°©ì‹)""" import time start_time = time.time() print(f"\nğŸ” ===== Flash ë§¥ë½ ë¶„ì„ ì‹œì‘ =====") print(f"ğŸ“ ì›ë³¸ ì¿¼ë¦¬: '{current_msg}'") print(f"ğŸ“š ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬: {len(conversation_history)}ê°œ ë©”ì‹œì§€") # ìµœê·¼ 6ê°œ ë©”ì‹œì§€ë§Œ ì‚¬ìš© (user-assistant 3ìŒìœ¼ë¡œ ë” ë§ì€ ë§¥ë½ í™•ë³´) recent_history = conversation_history[-6:] if len(conversation_history) >= 6 else conversation_history print(f"ğŸ“Š ë¶„ì„ì— ì‚¬ìš©í•  ìµœê·¼ ë©”ì‹œì§€: {len(recent_history)}ê°œ") # ì´ì „ ë©”ì‹œì§€ê°€ ì „í˜€ ì—†ì„ ë•Œë§Œ ì›ë³¸ ë°˜í™˜ if len(recent_history) == 0: print("âš ï¸ Flash ë¶„ì„ ìŠ¤í‚µ: íˆìŠ¤í† ë¦¬ ì—†ìŒ (ì²« ë©”ì‹œì§€)") print(f"â±ï¸ Flash ë¶„ì„ ìŠ¤í‚µ ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ\n") return current_msg # ë¶„ì„í•  ë§¥ë½ ì—†ìŒ # í˜„ì¬ ë©”ì‹œì§€ë§Œ ìˆëŠ” ê²½ìš° (ëŒ€í™” ì‹œì‘) if len(recent_history) == 1 and recent_history[0].content == current_msg: print("âš ï¸ Flash ë¶„ì„ ìŠ¤í‚µ: í˜„ì¬ ë©”ì‹œì§€ë§Œ ì¡´ì¬") print(f"â±ï¸ Flash ë¶„ì„ ìŠ¤í‚µ ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ\n") return current_msg try: # ê°„ì†Œí™”ëœ í”„ë¡¬í”„íŠ¸ êµ¬ì„± context_text = "" for i, msg in enumerate(recent_history): content = msg.content[:300] if len(msg.content) > 300 else msg.content # 300ì ì œí•œ context_text += f"{msg.role}: {content}\n" prompt_text = f"""Recent conversation context:{context_text}Current question: "{current_msg}"Analyze the conversation and perform one of the following:1. If the current question refers to previous context: - Replace pronouns and references with specific terms - Reformulate into a clear search query suitable for document retrieval - Example: "How does it work?" â†’ "Gemini API functionality" - Example: "What about the performance?" â†’ "Gemini model performance metrics"2. If it's an independent new question: - Return the original question as-isExamples:- Context reference: "What's the difference with the old method?" â†’ "new SDK vs legacy SDK differences"
   íˆìŠ¤í† ë¦¬ë¥¼ LLMì— ì „ë‹¬
   Claudeì™€ Gemini ëª¨ë‘ íˆìŠ¤í† ë¦¬ë¥¼ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤:
   claude.pyLines 818-822 # Flashë¡œ ë§¥ë½ ë¶„ì„ ë° ê²€ìƒ‰ì–´ ê°œì„  print("\nğŸ¯ ===== Flash ë§¥ë½ ë¶„ì„ íš¨ê³¼ì„± ì¸¡ì • =====") print(f"ğŸ“‹ í˜„ì¬ ëŒ€í™” ë‹¨ê³„: {len(conversation_history[conversation_id])}ë²ˆì§¸ ë©”ì‹œì§€") enhanced_query = await analyze_context_with_flash(request.message, conversation_history[conversation_id])
2. ì´ì›ƒ ì²­í¬ í™•ì¥ (ì•ë’¤ nê°œ)
   ì„¤ì •ê°’
   rag_config.pyLines 6-12
   class RAGConfig: """RAG ê²€ìƒ‰ ê´€ë ¨ ì„¤ì •""" # ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ ì„¤ì • MAX_EXPANDED_RESULTS = 80 # ìµœëŒ€ ë°˜í™˜ ê²°ê³¼ ìˆ˜ NEIGHBOR_WINDOW = 5 # ì´ì›ƒ ì²­í¬ í™•ì¥ ë²”ìœ„ (ì•ë’¤ 5ê°œì”©)
   ì´ì›ƒ ì²­í¬ ì¡°íšŒ í•¨ìˆ˜
   rag_processor.pyLines 13-90
   def fetch_neighbor_chunks(doc_id: int, center_idx: int, window: int = None) -> List[Dict]: """ ê°™ì€ ë¬¸ì„œì—ì„œ chunk_indexÂ±window ë²”ìœ„ ì²­í¬ë“¤ì„ ê°€ì ¸ì˜¨ë‹¤ Args: doc_id: ë¬¸ì„œ ID center_idx: ì¤‘ì‹¬ ì²­í¬ ì¸ë±ìŠ¤ window: ì´ì›ƒ ë²”ìœ„ (ê¸°ë³¸ê°’: config.NEIGHBOR_WINDOW) Returns: ì´ì›ƒ ì²­í¬ ë¦¬ìŠ¤íŠ¸ """ from app.router.vector.opensearch import opensearch_client, INDEX_NAME try: window = window or config.NEIGHBOR_WINDOW gte = max(center_idx - window, 0) lte = center_idx + window print(f"ğŸ” [ì´ì›ƒ ì²­í¬ ì¡°íšŒ] doc_id={doc_id}, center={center_idx}, range=[{gte}, {lte}]") body = { "size": (window _ 2 + 1) _ 5, # ì¤‘ë³µ ë°ì´í„° ê³ ë ¤í•´ì„œ 5ë°°ë¡œ ì¶©ë¶„íˆ í¬ê²Œ "sort": [{"chunk_index": "asc"}, {"created_at": "desc"}], # chunk_index ìš°ì„ , ìµœì‹ ìˆœ "\_source": {"excludes": ["embedding"]}, "query": { "bool": { "filter": [ {"term": {"document_id": doc_id}}, {"range": {"chunk_index": {"gte": gte, "lte": lte}}} ] } } } response = opensearch_client.search(index=INDEX_NAME, body=body) hits = response.get("hits", {}).get("hits", []) chunk_indices = [hit.get("_source", {}).get("chunk_index") for hit in hits] print(f" â†’ ì¡°íšŒëœ ì²­í¬: {len(hits)}ê°œ, chunk_index={chunk_indices}") # ì¤‘ë³µ ì œê±°: ê°™ì€ chunk_indexëŠ” ìµœì‹  ê²ƒë§Œ ìœ ì§€ seen_indices = set() neighbors = [] for hit in hits: source = hit.get("\_source", {}) chunk_idx = source.get('chunk_index') if chunk_idx in seen_indices: continue seen_indices.add(chunk_idx) text_content = source.get('text', '') neighbor = { 'chunk_id': hit.get('\_id'), 'score': 0.0, 'text': text_content, 'document_id': source.get('document_id'), 'page_number': source.get('page_number'), 'chunk_index': source.get('chunk_index'), 'file_name': source.get('original_filename', 'unknown'), 'source': source.get('source', ''), 'file_type': source.get('file_type', ''), 'file_path': source.get('file_path', ''), 'is_neighbor': True } neighbors.append(neighbor) unique_indices = [n.get('chunk_index') for n in neighbors] print(f" â†’ ì¤‘ë³µ ì œê±° í›„: {len(neighbors)}ê°œ, chunk_index={unique_indices}") return neighbors except Exception as e: print(f"âš ï¸ ì´ì›ƒ ì²­í¬ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}") return []
   ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ ì‹œ ì´ì›ƒ ì²­í¬ ìë™ í¬í•¨
   rag_processor.pyLines 93-166
   def process_search_results(search_results: List[Any], query_terms: List[str]) -> List[Dict]: """ ê²€ìƒ‰ ê²°ê³¼ + ì•ë’¤ ì´ì›ƒ ì²­í¬ ë¬´ì¡°ê±´ í¬í•¨ (ë‹¨ìˆœ ë²„ì „) Args: search_results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ query_terms: ì¿¼ë¦¬ ìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ë¯¸ì‚¬ìš©, í˜¸í™˜ì„± ìœ ì§€) Returns: ì²˜ë¦¬ëœ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ë”•ì…”ë„ˆë¦¬ í˜•íƒœ) """ if not search_results: return [] print("ğŸ¯ ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ ì‹œì‘ (ë‹¨ìˆœ ëª¨ë“œ: ì•ë’¤ ì²­í¬ ë¬´ì¡°ê±´ í¬í•¨)") expanded_results = [] seen_chunks = set() for result_obj in search_results: chunk_id = getattr(result_obj, 'chunk_id', None) doc_id = getattr(result_obj, 'document_id', None) chunk_idx = getattr(result_obj, 'chunk_index', None) key = chunk_id or f"{doc_id}:{chunk_idx}" if key in seen_chunks: continue seen_chunks.add(key) # ì›ë³¸ ê²°ê³¼ ì¶”ê°€ text_content = getattr(result_obj, 'text', '') result_dict = { 'chunk_id': chunk_id, 'score': getattr(result_obj, 'score', 0.0), 'text': text_content, 'document_id': doc_id, 'page_number': getattr(result_obj, 'page_number', None), 'chunk_index': chunk_idx, 'file_name': getattr(result_obj, 'file_name', 'unknown'), 'source': getattr(result_obj, 'source', ''), 'file_type': getattr(result_obj, 'file_type', ''), 'file_path': getattr(result_obj, 'file_path', ''), 'is_neighbor': False } expanded_results.append(result_dict) # ì•ë’¤ ì´ì›ƒ ì²­í¬ ë¬´ì¡°ê±´ ì¶”ê°€ if doc_id is not None and chunk_idx is not None: neighbors = fetch_neighbor_chunks(doc_id, chunk_idx, window=config.NEIGHBOR_WINDOW) for neighbor in neighbors: nb_key = neighbor.get('chunk_id') or f"{neighbor.get('document_id')}:{neighbor.get('chunk_index')}" if nb_key in seen_chunks: continue expanded_results.append(neighbor) seen_chunks.add(nb_key) # ìƒí•œì„  ì ìš© (ë‹¨ìˆœ ìŠ¬ë¼ì´ì‹±) if len(expanded_results) > config.MAX_EXPANDED_RESULTS: print(f" - ìƒí•œ ì œí•œ ì ìš©: {len(expanded_results)}ê°œ â†’ {config.MAX_EXPANDED_RESULTS}ê°œ") expanded_results = expanded_results[:config.MAX_EXPANDED_RESULTS] # í†µê³„ ì¶œë ¥ print(f"âœ… ìµœì¢… ë¬¸ì„œ ê²°ê³¼: {len(expanded_results)}ê°œ") neighbor_count = sum(1 for r in expanded_results if r.get('is_neighbor', False)) original_count = len(expanded_results) - neighbor_count print(f" - ì›ë³¸: {original_count}ê°œ, ì´ì›ƒ: {neighbor_count}ê°œ") for i, r in enumerate(expanded_results[:3], 1): chunk_type = "[ì´ì›ƒ]" if r.get('is_neighbor') else "[ì›ë³¸]" print(f" {i}. {r.get('file_name', 'unknown')} (p.{r.get('page_number', '?')}) {chunk_type} - ì ìˆ˜: {r.get('score', 0.0):.4f}") return expanded_results
   ìš”ì•½
   ì±„íŒ… íˆìŠ¤í† ë¦¬ í¬í•¨ ë°©ì‹
   S3ì—ì„œ ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¡œë“œ: load_conversation_from_s3()ë¡œ ì´ì „ ë©”ì‹œì§€ë“¤ì„ ê°€ì ¸ì˜´
   Flashë¡œ ë§¥ë½ ë¶„ì„: ìµœê·¼ 6ê°œ ë©”ì‹œì§€ë¥¼ ì‚¬ìš©í•´ ê²€ìƒ‰ì–´ ê°œì„  (ì˜ˆ: "ê·¸ê²ƒì€ ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?" â†’ "Gemini API ê¸°ëŠ¥")
   LLMì— íˆìŠ¤í† ë¦¬ ì „ë‹¬: conversation_historyë¥¼ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬
   ì´ì›ƒ ì²­í¬ í™•ì¥
   ì„¤ì •: NEIGHBOR_WINDOW = 5 (ì•ë’¤ 5ê°œì”©)
   ìë™ í™•ì¥: process_search_results()ê°€ ê° ê²€ìƒ‰ ê²°ê³¼ì— ëŒ€í•´ fetch_neighbor_chunks() í˜¸ì¶œ
   ë²”ìœ„: chunk_index Â± 5 ë²”ìœ„ì˜ ì²­í¬ë¥¼ ê°™ì€ ë¬¸ì„œì—ì„œ ì¡°íšŒ
   ì¤‘ë³µ ì œê±°: ê°™ì€ chunk_indexëŠ” ìµœì‹  ê²ƒë§Œ ìœ ì§€
   ìµœëŒ€ ì œí•œ: MAX_EXPANDED_RESULTS = 80ê°œê¹Œì§€ í™•ì¥
   ì´ ë°©ì‹ìœ¼ë¡œ ê²€ìƒ‰ ê²°ê³¼ì˜ ë§¥ë½ì„ í™•ì¥í•˜ê³ , ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í™œìš©í•´ ê²€ìƒ‰ í’ˆì§ˆì„ ê°œì„ í•©ë‹ˆë‹¤.
