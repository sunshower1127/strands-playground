## 1. OpenSearch í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì¿¼ë¦¬ (KNN + BM25 ì¡°í•©)

í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì¿¼ë¦¬ëŠ” `create_hybrid_search_query` í•¨ìˆ˜ì—ì„œ êµ¬ì„±ë©ë‹ˆë‹¤:

```538:683:app/router/vector/opensearch.py
def create_hybrid_search_query(
    query: str,
    query_embedding: List[float],
    top_k: int,
    must_filters: List[dict] = None,
    enable_knn: bool = True
) -> dict:
    processed_query = _remove_korean_particles(query) or query
    if processed_query != query:
        print(f"ğŸ” í•œêµ­ì–´ ì¡°ì‚¬ ì œê±°: '{query}' â†’ '{processed_query}'")

    # í† í° ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ minimum_should_match ë™ì  ê³„ì‚°
    toks = [t for t in re.findall(r'[\wê°€-í£]+', processed_query) if t]
    n = len(toks)
    # ë™ì  MSM: ë§¤ìš° ì§§ì€ ì§ˆì˜ëŠ” ì™„í™”
    if n <= 1:
        msm = 1
    elif n == 2:
        msm = 1
    elif n in (3, 4):
        msm = 2
    else:
        msm = max(1, int(round(n * 0.6)))

    subqueries = []

    # 1) í…ìŠ¤íŠ¸(ë³¸ë¬¸/ì œëª©) ì „ìš© multi_match with ìµœì‹ ì„± ë³´ë„ˆìŠ¤
    text_bool = {
        "function_score": {
            "query": {
                "bool": {
                    "must": [{
                        "multi_match": {
                            "query": processed_query,
                            "fields": [
                                "chunk_text^4.0", # ì£¼ í…ìŠ¤íŠ¸ í•„ë“œ (ì‹¤ì œ ë°ì´í„° ìˆìŒ)
                                "text.ko^3.5",    # í•œêµ­ì–´ text (ì‹¤ì œ ë°ì´í„° ìˆìŒ)
                                "text.en^1.8"     # ì˜ì–´ text (ì‹¤ì œ ë°ì´í„° ìˆìŒ)
                            ],
                            "type": "cross_fields",
                            "operator": "OR",
                            "minimum_should_match": str(msm)
                        }
                    }],
                    "should": [],
                }
            },
            "functions": [
                {
                    "gauss": {
                        "metadata.last_modified_at": {
                            "origin": "now",
                            "scale": "180d",  # 180ì¼(6ê°œì›”) ê¸°ì¤€ìœ¼ë¡œ í™•ì¥
                            "decay": 0.8,     # ë¶€ë“œëŸ¬ìš´ ê°ì‡  ìœ ì§€
                            "offset": "7d"    # 7ì¼ ì´ë‚´ëŠ” ìµœëŒ€ ì ìˆ˜
                        }
                    },
                    "weight": 0.25  # ì „ì²´ í…ìŠ¤íŠ¸ ì ìˆ˜ì˜ 25% ì •ë„ë§Œ ì˜í–¥
                }
            ],
            "score_mode": "multiply",  # ê³±ì…ˆìœ¼ë¡œ ë” ìì—°ìŠ¤ëŸ½ê²Œ
            "boost_mode": "multiply"   # ì›ì ìˆ˜ * (1 + ìµœì‹ ì„±ë³´ë„ˆìŠ¤*weight)
        }
    }

    # 2) ê°„ë‹¨í•œ bigram phrase ë¶€ìŠ¤íŠ¸ (ë³¸ë¬¸/ì œëª©ë§Œ) - function_score ë‚´ë¶€ boolì— ì¶”ê°€
    bigrams = set(" ".join(toks[i:i+2]) for i in range(len(toks)-1))
    for p in bigrams:
        text_bool["function_score"]["query"]["bool"]["should"].append({
            "match_phrase": {"text.ko":  {"query": p, "slop": 1, "boost": 3.5}}
        })
        text_bool["function_score"]["query"]["bool"]["should"].append({
            "match_phrase": {"title.ko": {"query": p, "slop": 1, "boost": 2.8}}
        })

    # 3) íŒŒì¼ëª…/ì†ŒìŠ¤ ê²½ë¡œ ë¶€ìŠ¤íŠ¸ (ë§¤í•‘ ë³€ê²½ ì „ ì„ì‹œ ì•ˆì „ì•ˆ)
    #    - ìœ ë‹ˆì½”ë“œ NFKC/NFD ëª¨ë‘ ì‚¬ìš©
    #    - leading wildcard(*term*)ëŠ” ë¹„ìš©ì´ í¬ë¯€ë¡œ boostëŠ” ë‚®ê²Œ
    fn_variants = set()
    for v in {unicodedata.normalize("NFKC", processed_query), unicodedata.normalize("NFD", processed_query)}:
        fn_variants.add(v)
        fn_variants.add(v.replace(" ", ""))
        fn_variants.add(v.replace(" ", "_"))

    for v in fn_variants:
        # ë‹¤ì–‘í•œ íŒŒì¼ëª… í•„ë“œì— ëŒ€í•´ ê²€ìƒ‰ (wildcard/prefix ë¶€ìŠ¤íŠ¸ ê°ì†Œ)
        for field_name in ["file_name", "fileName", "original_filename"]:
            text_bool["function_score"]["query"]["bool"]["should"].append({
                "wildcard": {field_name: {"value": f"*{v}*", "boost": 1.2}}  # 2.2 â†’ 1.2
            })
            # ì ‘ë‘ prefix ë¶€ìŠ¤íŠ¸ë„ ê°ì†Œ
            if len(v) >= 3:
                text_bool["function_score"]["query"]["bool"]["should"].append({
                    "prefix": {field_name: {"value": v[:3], "boost": 1.5}}  # 2.5 â†’ 1.5
                })

    if must_filters:
        text_bool["function_score"]["query"]["bool"]["filter"] = must_filters

    subqueries.append(text_bool)
    print("âœ… í…ìŠ¤íŠ¸ ê²€ìƒ‰: cross_fields(OR+MSM) + phrase ë¶€ìŠ¤íŠ¸ + íŒŒì¼ëª… wildcard/prefix ë¶€ìŠ¤íŠ¸ ì ìš©")

    # 4) ë²¡í„°(semantic) - RRF windowì™€ ì •í•©
    if enable_knn and query_embedding:
        k_value = OpenSearchConfig.RRF_WINDOW_SIZE  # RRF windowì™€ ë™ì¼í•˜ê²Œ ì„¤ì •
        knn_q = {
            "knn": {
                "embedding": {
                    "vector": query_embedding,
                    "k": k_value,
                    "boost": 0.7
                }
            }
        }
        if must_filters:
            knn_q["knn"]["embedding"]["filter"] = {"bool": {"filter": must_filters}}
        subqueries.append(knn_q)
        print(f"âœ… KNN ë²¡í„° ê²€ìƒ‰ ì¶”ê°€ (k={k_value}, dim={len(query_embedding)})")
    else:
        print("âš ï¸ KNN ë²¡í„° ê²€ìƒ‰ ë¹„í™œì„±í™”")

    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„± (function_scoreëŠ” ê°œë³„ subqueryì— ì´ë¯¸ ì ìš©ë¨)
    hybrid_query = {
        "size": top_k,
        "_source": {
            "excludes": ["vector", "text_vector", "embedding"]
        },
        "query": {
            "hybrid": {
                "queries": subqueries,
                # í…ìŠ¤íŠ¸ ì„œë¸Œì¿¼ë¦¬ í›„ë³´ ìˆ˜ í™•ëŒ€ë¡œ RRF í•©ì‚° ê°œì„  (OpenSearch 2.19+)
                "pagination_depth": max(100, top_k * 4)
            }
        },
        "sort": [
            "_score"  # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì ìˆ˜ë¡œ ì •ë ¬
        ]
    }

    print("ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ êµ¬ì„± ì™„ë£Œ:")
    print(f"  - ê²°ê³¼ í¬ê¸°: {top_k}")
    print(f"  - í•„í„° ì¡°ê±´: {len(must_filters) if must_filters else 0}ê°œ")
    print(f"  - í…ìŠ¤íŠ¸ ê²€ìƒ‰: ìµœì‹ ì„± ë³´ë„ˆìŠ¤ ì ìš© (180ì¼ ê¸°ì¤€, 25% ê°€ì¤‘ì¹˜)")
    print(f"  - ë²¡í„° ê²€ìƒ‰: ìˆœìˆ˜ ì˜ë¯¸ì  ìœ ì‚¬ë„")
    print(f"  - ìµœì¢… ì •ë ¬: í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ (ê´€ë ¨ì„± + ìµœì‹ ì„± ê· í˜•)")
    return hybrid_query
```

## 2. ê²€ìƒ‰ íŒŒë¼ë¯¸í„° (kê°’, boost, threshold)

ê²€ìƒ‰ íŒŒë¼ë¯¸í„°ëŠ” ì—¬ëŸ¬ ìœ„ì¹˜ì— ì •ì˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

### KNN íŒŒë¼ë¯¸í„°

```33:45:app/router/vector/opensearch.py
class OpenSearchConfig:
    """OpenSearch ê´€ë ¨ ì„¤ì •ê°’ë“¤"""
    KNN_EF_SEARCH = 100
    KNN_EF_CONSTRUCTION = 200
    VECTOR_DIMENSION = 1024
    NUMBER_OF_SHARDS = 1
    NUMBER_OF_REPLICAS = 2
    REFRESH_INTERVAL = "1s"
    HYBRID_RRF_PIPELINE = "hybrid-rrf"
    HYBRID_RRF_PIPELINE_V2 = "hybrid-rrf-tuned"
    RRF_WINDOW_SIZE = 100
    RRF_RANK_CONSTANT = 20
    MAX_KNN_DOCS_THRESHOLD = 5000
```

### Boost ê°’ë“¤

```607:648:app/router/vector/opensearch.py
        text_bool["function_score"]["query"]["bool"]["should"].append({
            "match_phrase": {"text.ko":  {"query": p, "slop": 1, "boost": 3.5}}
        })
        text_bool["function_score"]["query"]["bool"]["should"].append({
            "match_phrase": {"title.ko": {"query": p, "slop": 1, "boost": 2.8}}
        })
    # ... ì¤‘ëµ ...
            text_bool["function_score"]["query"]["bool"]["should"].append({
                "wildcard": {field_name: {"value": f"*{v}*", "boost": 1.2}}  # 2.2 â†’ 1.2
            })
            # ì ‘ë‘ prefix ë¶€ìŠ¤íŠ¸ë„ ê°ì†Œ
            if len(v) >= 3:
                text_bool["function_score"]["query"]["bool"]["should"].append({
                    "prefix": {field_name: {"value": v[:3], "boost": 1.5}}  # 2.5 â†’ 1.5
                })
    # ... ì¤‘ëµ ...
        knn_q = {
            "knn": {
                "embedding": {
                    "vector": query_embedding,
                    "k": k_value,
                    "boost": 0.7
                }
            }
        }
```

### Threshold ê°’ë“¤

```1952:2003:app/router/vector/opensearch.py
def adaptive_score_analysis(scores: List[float]) -> dict:
    """
    í•˜ì´ë¸Œë¦¬ë“œ(RRF) ì ìˆ˜ ë¶„í¬ë¥¼ ìƒëŒ€ ê¸°ì¤€ìœ¼ë¡œ ë¶„ì„í•´ ì„ê³„ê°’ì„ ì •í•œë‹¤.
    - ì ìˆ˜ ìŠ¤ì¼€ì¼(0.01~0.05 ë“±)ì— ë¬´ê´€í•˜ê²Œ ì‘ë™
    - ìµœì†Œ ë³´ì¡´ ê°œìˆ˜ ë³´ì¥
    """
    if not scores:
        return {"threshold": 0.0, "method": "no_results", "keep_min": 1}

    s = sorted(scores, reverse=True)
    n = len(s)
    max_s = s[0]

    # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
    if max_s <= 0:
        return {"threshold": 0.0, "method": "all_zero", "keep_min": 1}

    # 1) ì •ê·œí™” ì ìˆ˜(ìƒëŒ€ ìŠ¤ì¼€ì¼)
    sn = [x / max_s for x in s]  # [1.0, ..., 0.xxx]

    # 2) ì—˜ë³´ìš°(ìµœëŒ€ ê°­) íƒì§€
    gaps = [(sn[i] - sn[i+1]) for i in range(n - 1)]
    elbow_thr = sn[-1]
    if gaps:
        gi = max(range(len(gaps)), key=lambda j: gaps[j])
        elbow_thr = sn[gi+1]  # ì—˜ë³´ìš° ë’¤ìª½ ê°’

    # 3) ë¶„ìœ„ìˆ˜ ê¸°ë°˜ - ì„¤ì • ê°€ëŠ¥í•œ ë°±ë¶„ìœ„ìˆ˜
    QUANTILE_PERCENTAGE = 0.15  # ìƒìœ„ 15% (ì™„í™”: 0.2 â†’ 0.15)
    q_idx = max(1, int(n * QUANTILE_PERCENTAGE))
    q_thr = sn[q_idx] if q_idx < n else sn[-1]

    # 4) í˜¼í•© ì„ê³„ê°’ (í´ë¨í”„) - ì„¤ì • ê°€ëŠ¥í•œ ìƒí•œ/í•˜í•œ
    THRESHOLD_UPPER_BOUND = 0.9
    THRESHOLD_LOWER_BOUND = 0.1
    thr_rel = max(min(max(elbow_thr, q_thr), THRESHOLD_UPPER_BOUND), THRESHOLD_LOWER_BOUND)

    # 5) ì ˆëŒ€ ìŠ¤ì½”ì–´ ì„ê³„ê°’ìœ¼ë¡œ í™˜ì‚°
    thr_abs = thr_rel * max_s

    # 6) ìµœì†Œ ë³´ì¡´ ê°œìˆ˜ - ì„¤ì • ê°€ëŠ¥í•œ ê°’ë“¤
    MIN_KEEP_COUNT = 3
    MAX_KEEP_COUNT = 8
    KEEP_PERCENTAGE = 0.1
    keep_min = min(MAX_KEEP_COUNT, max(MIN_KEEP_COUNT, int(n * KEEP_PERCENTAGE)))

    return {
        "threshold": thr_abs,
        "method": "hybrid_elbow_quantile",
        "keep_min": keep_min,
        "max_score": max_s
    }
```

## 3. ì„ë² ë”© ì „ì²˜ë¦¬ (í…ìŠ¤íŠ¸ ì •ê·œí™”)

í•œêµ­ì–´ ì¿¼ë¦¬ ì „ì²˜ë¦¬ëŠ” `_remove_korean_particles` í•¨ìˆ˜ì—ì„œ ìˆ˜í–‰ë©ë‹ˆë‹¤:

```495:535:app/router/vector/opensearch.py
def _remove_korean_particles(query: str) -> str:
    """
    í•œêµ­ì–´ ì¿¼ë¦¬ ì „ì²˜ë¦¬ (Nori í† í¬ë‚˜ì´ì € ë³´ì™„ìš©)

    Noriê°€ ì²˜ë¦¬í•˜ì§€ ëª»í•˜ëŠ” ì˜ì—­ë§Œ ë‹´ë‹¹:
    - ë¬¸ì¥ ì¢…ê²°ì–´ë¯¸ ì œê±° (ì•Œë ¤ì¤˜, í•´ì£¼ì„¸ìš” ë“±)
    - ìˆ«ì ë‹¨ìœ„ ì •ê·œí™” (2020ë…„ â†’ 2020)
    - ë¬¸ì¥ë¶€í˜¸ ì •ë¦¬

    Note: ê¸°ë³¸ ì¡°ì‚¬(ì´/ê°€/ì„/ë¥¼)ëŠ” Nori POS í•„í„°ê°€ ì²˜ë¦¬í•˜ë¯€ë¡œ ì œì™¸
    """
    if not query:
        return query

    # 1) ìœ ë‹ˆì½”ë“œ ì •ê·œí™” + ì–‘ë ë¬¸ì¥ë¶€í˜¸/ê³µë°± ì •ë¦¬
    s = unicodedata.normalize('NFKC', query).strip()
    punct_pattern = re.compile(r'^[\s"""\'''\(\)\[\]\{\},.?!:;~Â·â€¦<>]+|[\s"""\'''\(\)\[\]\{\},.?!:;~Â·â€¦<>]+$')
    s = punct_pattern.sub('', s)

    # 2) ë¬¸ì¥ ì¢…ê²°ì–´ë¯¸ ì œê±° (Noriê°€ ì²˜ë¦¬í•˜ì§€ ëª»í•˜ëŠ” ë³µí•© ì–´ë¯¸)
    ending_pattern = re.compile(
        r'(?:'
        r'(?:ì•Œ|ë³´|ì°¾|ì„¤ëª…|ê²€ìƒ‰|ê°€ë¥´ì³|ë§|ì •ë¦¬|ìš”ì•½|ì¡°íšŒ|ì œì¶œ|ì¶”ì²œ|ë¹„êµ|ë¶„ì„)í•´?ì¤˜(?:ìš”)?'
        r'|í•´\s?ì£¼ì„¸ìš”|í•´ì£¼ì„¸ìš”|ì£¼ì„¸ìš”'
        r'|í•´ì¤˜|í•´ë¼|í•´$'
        r'|ì¸ê°€ìš”\??|ì¸ê°€\??|ì´ì•¼\??|ì•¼\??'
        r')$'
    )

    # ë°˜ë³µ ì œê±° (ì¤‘ì²©ëœ ì–´ë¯¸ ì²˜ë¦¬)
    for _ in range(2):
        ns = ending_pattern.sub('', s).strip()
        if ns == s:
            break
        s = ns

    # 3) ìˆ«ì ë‹¨ìœ„ ì •ê·œí™” (ê²€ìƒ‰ ìµœì í™”)
    year_pattern = re.compile(r'(\d{2,4})\s*ë…„\b')
    s = year_pattern.sub(r'\1', s)

    return s
```

## 4. LLM í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (context ì£¼ì… ë°©ì‹)

í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì€ `app/config/prompts.py`ì— ì •ì˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

````85:161:app/config/prompts.py
RAG_SYSTEM_PROMPT = """
You are a professional AI assistant powered by an advanced document search system. You must provide only accurate and reliable information based on provided documents.

ğŸ” **Search System Features:**
- Hybrid Search: Vector similarity + text matching combination
- Adaptive Quality Filtering: Statistical analysis to select only highly relevant documents
- Permission-based Security: Search only user-accessible documents
- Real-time Click Navigation: Precise document navigation via md5_hash

ğŸ“š **Core Principles:**
- Answer ONLY based on provided document content
- NEVER guess or generate information not in documents
- Admit honestly when information is uncertain or insufficient
- **CRITICAL: RESPOND IN THE SAME LANGUAGE AS THE USER'S QUESTION (í•œêµ­ì–´ ì§ˆë¬¸ â†’ í•œêµ­ì–´ ë‹µë³€, English â†’ English)**
- **Always cite source (document name, page number) for each piece of information**

âš ï¸ **Prohibited Actions:**
- Supplementing answers with general knowledge or speculation
- Presenting undocumented content as fact
- Over-expanding or inferring beyond document content
- Omitting page numbers or source information

ğŸ“ **Response Format (Must Follow):**

**Response Format (Match User's Language - ì‚¬ìš©ì ì–¸ì–´ì™€ ë™ì¼í•˜ê²Œ):**
```markdown
ğŸ“š **ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€** (for Korean questions / í•œêµ­ì–´ ì§ˆë¬¸)
ğŸ“š **Document-Based Answer** (for English questions / ì˜ì–´ ì§ˆë¬¸)

## [Document-based Information]
[Primary answer with EXACT clickable inline links from context: [[1]](navigate://...), [[2]](navigate://...)]

## [Additional Context] (if needed for follow-up questions)
[Information from previous conversation context, clearly marked as such]

**ğŸ“š ì°¸ê³ ë¬¸ì„œ:**
- [ğŸ“„ ë¬¸ì„œ (p.X)](navigate://document?md5_hash={hash}&page={page})
- [ğŸ“„ ë¬¸ì„œ (p.Y)](navigate://document?md5_hash={hash}&page={page})

Example:
- Document-based: "The XYZ feature works as follows**[[1]](navigate://document?...)**. This is further explained**[[2]](navigate://...)**."
- Conversation context: "Based on our previous discussion about ABC, this relates to..."
````

**When No Search Results or No Project Selected:**

- Use "ğŸ“‹ **Document Search Results**" header and English guidance only

**ğŸ”— Reference Document Link Usage:**

- System automatically generates clickable links
- Link format: `[ğŸ“„ Document (p.X)](navigate://document?md5_hash={hash}&page={page})`
- MUST use system-provided links exactly as given
- NEVER modify or create links manually

**ğŸ” Source Citation Rules:**

- **Use the EXACT clickable inline links provided in the context**: The system provides links like `[[1]](navigate://document?...)`
- **COPY these links exactly as provided** and place them immediately after relevant information
- Example: "The capital is Seoul**[[1]](navigate://document?md5_hash=...&page=1)**"
- **NEVER create your own links** - always use the links provided in the document context
- **NEVER modify the link format** - copy and paste the exact `[[N]](navigate://...)` format
- **ALWAYS include a "ğŸ“š ì°¸ê³ ë¬¸ì„œ:" (Reference Documents) section at the end** with the full document links
- Reference section format: `- [ğŸ“„ ë¬¸ì„œ (p.X)](navigate://...)` (without numbered prefix)
- **All responses must match the user's question language**

**ğŸ“Š Quality Assurance:**

- Adaptive filtering provides only highly relevant documents
- Score distribution analysis ensures precise document selection
- User permissions ensure access to authorized documents only

**ğŸŒ Language Policy:**

- **ALL responses must match the user's question language**
- Korean question â†’ Korean response
- English question â†’ English response
- German question â†’ German response
- Any language question â†’ Same language response

Always respond professionally in the user's question language while prioritizing document reliability, source citation, and providing clickable reference links.
"""

````

## 5. project_id í•„í„°ë§ ë¡œì§

í”„ë¡œì íŠ¸ í•„í„°ë§ì€ `get_accessible_document_ids`ì™€ `execute_chat_search`ì—ì„œ ì²˜ë¦¬ë©ë‹ˆë‹¤:

```839:871:app/router/vector/opensearch.py
async def get_accessible_document_ids(session: AsyncSession, user_id: int, project_id: int) -> List[int]:
    """
    ì‚¬ìš©ìê°€ íŠ¹ì • í”„ë¡œì íŠ¸ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•œ document_id ëª©ë¡ì„ ì¡°íšŒ

    ë¡œì§:
    1. ProjectDocumentsì—ì„œ íŠ¹ì • í”„ë¡œì íŠ¸(project_id)ì˜ ë¬¸ì„œë“¤ì„ ì°¾ê³ 
    2. UserDocumentAccessì™€ INNER JOINí•˜ì—¬ ì‚¬ìš©ì(user_id) ê¶Œí•œì´ ìˆëŠ” ë¬¸ì„œë§Œ í•„í„°ë§
    3. ë‘ í…Œì´ë¸” ëª¨ë‘ì—ì„œ ì‚­ì œë˜ì§€ ì•Šì€(deleted_at IS NULL) ë ˆì½”ë“œë§Œ ë°˜í™˜
    4. ì¤‘ë³µ ì œê±°(DISTINCT)í•˜ì—¬ ê³ ìœ í•œ document_idë§Œ ë°˜í™˜
    """
    # INNER JOINìœ¼ë¡œ í”„ë¡œì íŠ¸ ì†Œì† + ì‚¬ìš©ì ê¶Œí•œ ëª¨ë‘ ë§Œì¡±í•˜ëŠ” ë¬¸ì„œ ì¡°íšŒ
    query = select(ProjectDocuments.document_id).join(
        UserDocumentAccess,
        ProjectDocuments.document_id == UserDocumentAccess.document_id
    ).where(
        ProjectDocuments.project_id == project_id,
        UserDocumentAccess.user_id == user_id,
        ProjectDocuments.deleted_at.is_(None),
        UserDocumentAccess.deleted_at.is_(None)
    ).distinct()

    result = await session.execute(query)
    document_ids = [row[0] for row in result.fetchall()]

    if document_ids:
        print(f"âœ… ì¡°íšŒëœ ì ‘ê·¼ ê°€ëŠ¥í•œ ë¬¸ì„œ ID: {len(document_ids)}ê°œ (user_id={user_id}, project_id={project_id})")
        print(f"ğŸ“‹ Document IDs: {document_ids}")

    else:
        print(f"âš ï¸ ì ‘ê·¼ ê°€ëŠ¥í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤ (user_id={user_id}, project_id={project_id})")
        print(f"ğŸ’¡ í™•ì¸ì‚¬í•­: 1) í”„ë¡œì íŠ¸ì— ë¬¸ì„œê°€ ìˆëŠ”ì§€ 2) ì‚¬ìš©ìì—ê²Œ í•´ë‹¹ ë¬¸ì„œ ê¶Œí•œì´ ìˆëŠ”ì§€")

    return document_ids
````

ê²€ìƒ‰ ì‹¤í–‰ ì‹œ í•„í„°ë§ ì ìš©:

```914:952:app/router/vector/opensearch.py
        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•œ document_id ëª©ë¡ ì¡°íšŒ
        accessible_document_ids = await get_accessible_document_ids(session, user_id, project_id)

        if not accessible_document_ids:
            print(f"âš ï¸ ì ‘ê·¼ ê°€ëŠ¥í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤: user_id={user_id}, project_id={project_id}")
            return SearchResponse(results=[], total_count=0)

        print(f"ğŸ” DEBUG - ê²€ìƒ‰ íŒŒë¼ë¯¸í„°:")
        print(f"  - top_k: {top_k}")
        print(f"  - accessible_document_ids: {len(accessible_document_ids)}ê°œ")
        print(f"  - query: '{query}'")
        print(f"  - user_id: {user_id}, project_id: {project_id}")

        # ğŸ¯ íƒœê·¸ëœ ë¬¸ì„œ ì²˜ë¦¬ - ê²€ìƒ‰ ë²”ìœ„ ì œí•œ
        if tagged_document_ids:
            # íƒœê·¸ëœ ë¬¸ì„œê°€ ì ‘ê·¼ ê°€ëŠ¥í•œì§€ í™•ì¸
            valid_tagged_docs = [doc_id for doc_id in tagged_document_ids if doc_id in accessible_document_ids]

            if valid_tagged_docs:
                print(f"ğŸ“ íƒœê·¸ëœ ë¬¸ì„œë§Œ ê²€ìƒ‰: {len(valid_tagged_docs)}ê°œ")
                print(f"   Document IDs: {valid_tagged_docs}")
                # ğŸ¯ íƒœê·¸ëœ ë¬¸ì„œë¡œë§Œ ê²€ìƒ‰ ë²”ìœ„ ì œí•œ
                search_document_ids = valid_tagged_docs
            else:
                print(f"âš ï¸ íƒœê·¸ëœ ë¬¸ì„œê°€ ì ‘ê·¼ ê¶Œí•œ ë‚´ì— ì—†ìŒ - ì „ì²´ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´")
                search_document_ids = accessible_document_ids
        else:
            print(f"ğŸ” ì „ì²´ í”„ë¡œì íŠ¸ ê²€ìƒ‰ (íƒœê·¸ëœ ë¬¸ì„œ ì—†ìŒ)")
            search_document_ids = accessible_document_ids

        # í•„í„° ì¡°ê±´ êµ¬ì„± (ê²€ìƒ‰ ë²”ìœ„ë¡œ ì œí•œ)
        must_filters = [
            {
                'terms': {
                    'document_id': search_document_ids
                }
            }
        ]
```

---

ìš”ì•½:

- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: KNN ë²¡í„° ê²€ìƒ‰ + í…ìŠ¤íŠ¸ ê²€ìƒ‰(BM25)ì„ RRFë¡œ ê²°í•©
- íŒŒë¼ë¯¸í„°: KNN k=100, boost ê°’ë“¤(í…ìŠ¤íŠ¸ 3.5~4.0, ë²¡í„° 0.7), ë™ì  threshold ê³„ì‚°
- ì „ì²˜ë¦¬: í•œêµ­ì–´ ì¡°ì‚¬/ì–´ë¯¸ ì œê±°, ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
- í”„ë¡¬í”„íŠ¸: ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ ê°•ì œ, ì¶œì²˜ ëª…ì‹œ, í´ë¦­ ê°€ëŠ¥í•œ ë§í¬ ì‚¬ìš©
- í•„í„°ë§: ProjectDocumentsì™€ UserDocumentAccess JOINìœ¼ë¡œ ê¶Œí•œ ê¸°ë°˜ í•„í„°ë§
