# PLAN 3: ë¹„êµ í‰ê°€ ë° íŠœë‹

## 3.1 ê²°ê³¼ ë¹„êµ íŒŒì´í”„ë¼ì¸

### A/B ê²°ê³¼ ë³‘í•©
```python
# scripts/compare.py
import pandas as pd

def merge_results():
    basic = load_json("data/results/basic_results.json")
    agent = load_json("data/results/agent_results.json")

    merged = []
    for b, a in zip(basic["results"], agent["results"]):
        merged.append({
            "id": b["id"],
            "question": b["question"],
            "answer_basic": b["answer"],
            "answer_agent": a["answer"],
            "latency_basic_ms": b["latency_ms"],
            "latency_agent_ms": a["latency_ms"],
            "tokens_basic": b["tokens_used"],
            "tokens_agent": a["tokens_used"],
            "tool_calls": a["tool_call_count"],
            "human_eval": "",      # ì‚¬ëŒì´ ì±„ìš¸ ì»¬ëŸ¼
            "llm_eval": "",        # LLMì´ ì±„ìš¸ ì»¬ëŸ¼
            "winner": "",          # basic / agent / tie
            "notes": "",           # íŠ¹ì´ì‚¬í•­ ë©”ëª¨
        })

    # CSVë¡œ ì €ì¥ (ì‚¬ëŒ í‰ê°€ìš©)
    df = pd.DataFrame(merged)
    df.to_csv("data/results/comparison.csv", index=False)

    return merged
```

### ê²°ê³¼ íŒŒì¼ í¬ë§·
```csv
id,question,answer_basic,answer_agent,latency_basic_ms,latency_agent_ms,tokens_basic,tokens_agent,tool_calls,human_eval,llm_eval,winner,notes
1,"ì§ˆë¬¸1","ë‹µë³€A","ë‹µë³€B",1200,3500,400,1200,2,"","","",""
2,"ì§ˆë¬¸2","ë‹µë³€A","ë‹µë³€B",1100,2800,380,950,1,"","","",""
...
```

### ì‚¬ëŒ í‰ê°€ ê°€ì´ë“œ
```markdown
## í‰ê°€ ê¸°ì¤€

ê° ì§ˆë¬¸ì— ëŒ€í•´ ë‘ ë‹µë³€ì„ ë¹„êµí•˜ê³  ë‹¤ìŒì„ ê¸°ë¡:

### human_eval ì»¬ëŸ¼
- "A": Basicì´ ë” ì¢‹ìŒ
- "B": Agentê°€ ë” ì¢‹ìŒ
- "T": ë™ì  (ë‘˜ ë‹¤ ë¹„ìŠ·)
- "X": ë‘˜ ë‹¤ ë¶€ì ì ˆ

### winner ì»¬ëŸ¼
- ìµœì¢… ìŠ¹ì ê¸°ë¡ (human_evalê³¼ ë™ì¼í•˜ê²Œ ì‹œì‘)

### notes ì»¬ëŸ¼
- íŠ¹ì´ì‚¬í•­ ë©”ëª¨
- ì˜ˆ: "Agentê°€ ì¬ê²€ìƒ‰í•´ì„œ ë” ì •í™•í•œ ì •ë³´ ì°¾ìŒ"
- ì˜ˆ: "Basicì´ ë” ê°„ê²°í•˜ê³  ëª…í™•í•¨"
- ì˜ˆ: "ë‘˜ ë‹¤ í™˜ê° ë°œìƒ"
```

### LLM í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
```python
# src/eval/judge.py
from litellm import completion

JUDGE_PROMPT = """
ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ë‘ ê°œì˜ ë‹µë³€ì„ ë¹„êµ í‰ê°€í•´ì£¼ì„¸ìš”.

## ì§ˆë¬¸
{question}

## ë‹µë³€ A (Basic RAG)
{answer_a}

## ë‹µë³€ B (Agent RAG)
{answer_b}

## í‰ê°€ ê¸°ì¤€
1. ì •í™•ì„±: ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í–ˆëŠ”ê°€?
2. ì™„ì „ì„±: í•„ìš”í•œ ì •ë³´ë¥¼ ëª¨ë‘ í¬í•¨í–ˆëŠ”ê°€?
3. ëª…í™•ì„±: ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í–ˆëŠ”ê°€?
4. ê´€ë ¨ì„±: ë¶ˆí•„ìš”í•œ ì •ë³´ ì—†ì´ í•µì‹¬ë§Œ ë‹µë³€í–ˆëŠ”ê°€?

## ì¶œë ¥ í˜•ì‹ (JSON)
{{
  "winner": "A" | "B" | "tie",
  "scores": {{
    "A": {{"accuracy": 1-5, "completeness": 1-5, "clarity": 1-5, "relevance": 1-5}},
    "B": {{"accuracy": 1-5, "completeness": 1-5, "clarity": 1-5, "relevance": 1-5}}
  }},
  "reasoning": "íŒë‹¨ ê·¼ê±°ë¥¼ 1-2ë¬¸ì¥ìœ¼ë¡œ"
}}
"""

def evaluate_pair(question: str, answer_a: str, answer_b: str) -> dict:
    response = completion(
        model="vertex_ai/claude-3-sonnet",
        messages=[{
            "role": "user",
            "content": JUDGE_PROMPT.format(
                question=question,
                answer_a=answer_a,
                answer_b=answer_b,
            )
        }],
        response_format={"type": "json_object"},
    )
    return json.loads(response.choices[0].message.content)


def run_llm_evaluation(comparison_file: str):
    df = pd.read_csv(comparison_file)

    for idx, row in df.iterrows():
        result = evaluate_pair(
            row["question"],
            row["answer_basic"],
            row["answer_agent"],
        )

        df.at[idx, "llm_eval"] = result["winner"]
        df.at[idx, "llm_scores"] = json.dumps(result["scores"])
        df.at[idx, "llm_reasoning"] = result["reasoning"]

        # ì‚¬ëŒ í‰ê°€ê°€ ì—†ìœ¼ë©´ LLM ê²°ê³¼ë¥¼ winnerë¡œ
        if pd.isna(row["human_eval"]) or row["human_eval"] == "":
            df.at[idx, "winner"] = result["winner"]

    df.to_csv(comparison_file, index=False)
    return df
```

### ë¶ˆì¼ì¹˜ ì¼€ì´ìŠ¤ í•˜ì´ë¼ì´íŠ¸
```python
def find_disagreements(df: pd.DataFrame) -> pd.DataFrame:
    """ì‚¬ëŒ í‰ê°€ì™€ LLM í‰ê°€ê°€ ë‹¤ë¥¸ ì¼€ì´ìŠ¤ ì¶”ì¶œ"""
    disagreements = df[
        (df["human_eval"] != "") &
        (df["human_eval"] != df["llm_eval"])
    ]

    print(f"ì´ {len(disagreements)}ê°œì˜ í‰ê°€ ë¶ˆì¼ì¹˜ ë°œê²¬")
    return disagreements
```

---

## 3.2 ë¶„ì„ ë° íŠœë‹

### í‰ê°€ ê²°ê³¼ ë¶„ì„
```python
# scripts/analyze.py
def analyze_results(comparison_file: str):
    df = pd.read_csv(comparison_file)

    # 1. ì „ì²´ ìŠ¹ë¥ 
    winner_counts = df["winner"].value_counts()
    print("=== ì „ì²´ ìŠ¹ë¥  ===")
    print(f"Basic ìŠ¹: {winner_counts.get('A', 0)}")
    print(f"Agent ìŠ¹: {winner_counts.get('B', 0)}")
    print(f"ë™ì : {winner_counts.get('tie', 0)}")

    # 2. ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„ (ì§ˆë¬¸ ìœ í˜•ë³„)
    if "category" in df.columns:
        print("\n=== ì¹´í…Œê³ ë¦¬ë³„ ìŠ¹ë¥  ===")
        category_analysis = df.groupby("category")["winner"].value_counts()
        print(category_analysis)

    # 3. ì„±ëŠ¥ ë¹„êµ
    print("\n=== ì„±ëŠ¥ ë¹„êµ ===")
    print(f"í‰ê·  ì‘ë‹µì‹œê°„ - Basic: {df['latency_basic_ms'].mean():.0f}ms")
    print(f"í‰ê·  ì‘ë‹µì‹œê°„ - Agent: {df['latency_agent_ms'].mean():.0f}ms")
    print(f"í‰ê·  í† í° - Basic: {df['tokens_basic'].mean():.0f}")
    print(f"í‰ê·  í† í° - Agent: {df['tokens_agent'].mean():.0f}")

    # 4. Agent ë„êµ¬ ì‚¬ìš© íŒ¨í„´
    print("\n=== Agent ë„êµ¬ ì‚¬ìš© ===")
    print(f"í‰ê·  ë„êµ¬ í˜¸ì¶œ: {df['tool_calls'].mean():.1f}íšŒ")
    print(f"ìµœëŒ€ ë„êµ¬ í˜¸ì¶œ: {df['tool_calls'].max()}íšŒ")

    # 5. ë¹„ìš© ì¶”ì •
    # Claude 3 Sonnet ê¸°ì¤€: input $3/1M, output $15/1M
    basic_cost = df['tokens_basic'].sum() * 0.000009  # ëŒ€ëµì  ì¶”ì •
    agent_cost = df['tokens_agent'].sum() * 0.000009
    print(f"\n=== ë¹„ìš© ì¶”ì • ===")
    print(f"Basic ì´ ë¹„ìš©: ${basic_cost:.4f}")
    print(f"Agent ì´ ë¹„ìš©: ${agent_cost:.4f}")
    print(f"Agent ì¶”ê°€ ë¹„ìš©: {(agent_cost/basic_cost - 1)*100:.1f}%")

    return df
```

### ë¶„ì„ ë³´ê³ ì„œ ìƒì„±
```python
def generate_report(df: pd.DataFrame, output_path: str):
    report = f"""
# RAG í‰ê°€ ë³´ê³ ì„œ

## ê°œìš”
- ì´ ì§ˆë¬¸ ìˆ˜: {len(df)}
- í‰ê°€ ì™„ë£Œ: {df['winner'].notna().sum()}

## ìŠ¹ë¥  ë¹„êµ
- Basic RAG: {(df['winner'] == 'A').sum()} ìŠ¹ ({(df['winner'] == 'A').mean()*100:.1f}%)
- Agent RAG: {(df['winner'] == 'B').sum()} ìŠ¹ ({(df['winner'] == 'B').mean()*100:.1f}%)
- ë™ì : {(df['winner'] == 'tie').sum()} ({(df['winner'] == 'tie').mean()*100:.1f}%)

## ì„±ëŠ¥ ë¹„êµ
| ì§€í‘œ | Basic | Agent | ì°¨ì´ |
|------|-------|-------|------|
| í‰ê·  ì‘ë‹µì‹œê°„ | {df['latency_basic_ms'].mean():.0f}ms | {df['latency_agent_ms'].mean():.0f}ms | +{df['latency_agent_ms'].mean() - df['latency_basic_ms'].mean():.0f}ms |
| í‰ê·  í† í° | {df['tokens_basic'].mean():.0f} | {df['tokens_agent'].mean():.0f} | +{df['tokens_agent'].mean() - df['tokens_basic'].mean():.0f} |

## Agentê°€ ìš°ìˆ˜í•œ ì¼€ì´ìŠ¤
{get_agent_wins_summary(df)}

## Basicì´ ìš°ìˆ˜í•œ ì¼€ì´ìŠ¤
{get_basic_wins_summary(df)}

## íŠœë‹ ê¶Œì¥ì‚¬í•­
{generate_recommendations(df)}
"""
    with open(output_path, "w") as f:
        f.write(report)
```

### íŠœë‹ ì‘ì—…
```python
# íŠœë‹ í¬ì¸íŠ¸

## 1. í”„ë¡¬í”„íŠ¸ íŠœë‹
- Agent ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê°œì„ 
- ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± ê°€ì´ë“œ ì¶”ê°€
- ë‹µë³€ í¬ë§· ì§€ì •

## 2. ê²€ìƒ‰ íŒŒë¼ë¯¸í„° íŠœë‹
- k (ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜): 3, 5, 7 ë¹„êµ
- ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜: k-NN vs í•˜ì´ë¸Œë¦¬ë“œ
- ì¬ê²€ìƒ‰ ì¡°ê±´ ì„¤ì •

## 3. ë„êµ¬ ì„¤ê³„ íŠœë‹
- ë„êµ¬ ì„¤ëª… ê°œì„  (Agentê°€ ë” ì˜ ì´í•´í•˜ë„ë¡)
- ì¶”ê°€ ë„êµ¬ í•„ìš” ì—¬ë¶€ ê²€í† 
- ë„êµ¬ ì¶œë ¥ í¬ë§· ìµœì í™”

## 4. ëª¨ë“œ ì „í™˜ ì „ëµ
- ì–´ë–¤ ì§ˆë¬¸ì— Agentë¥¼ ì“¸ì§€ ë¶„ë¥˜ê¸° ê°œë°œ
- ë¹„ìš© ëŒ€ë¹„ íš¨ê³¼ ì„ê³„ê°’ ì„¤ì •
```

### ë°˜ë³µ ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸
```python
# scripts/experiment.py
def run_experiment(config: dict, experiment_name: str):
    """ë‹¤ë¥¸ ì„¤ì •ìœ¼ë¡œ ì‹¤í—˜ ì‹¤í–‰"""

    # 1. ì„¤ì • ì ìš©
    update_config(config)

    # 2. í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    run_basic()  # ë˜ëŠ” run_agent()

    # 3. ê²°ê³¼ ì €ì¥ (ì‹¤í—˜ë³„ í´ë”)
    save_results(f"data/results/{experiment_name}/")

    # 4. ë¹„êµ ë¶„ì„
    compare_with_baseline(experiment_name)


# ì‹¤í—˜ ì˜ˆì‹œ
experiments = [
    {"name": "k3", "config": {"search_k": 3}},
    {"name": "k7", "config": {"search_k": 7}},
    {"name": "prompt_v2", "config": {"prompt_version": "v2"}},
]

for exp in experiments:
    run_experiment(exp["config"], exp["name"])
```

---

## ğŸ¯ Phase 3 ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] A/B ê²°ê³¼ ë³‘í•© ìŠ¤í¬ë¦½íŠ¸ ì™„ì„±
- [ ] comparison.csv ìƒì„±
- [ ] ì‚¬ëŒ í‰ê°€ ì™„ë£Œ (ì „ì²´ ë˜ëŠ” ìƒ˜í”Œë§)
- [ ] LLM í‰ê°€ ì‹¤í–‰ ì™„ë£Œ
- [ ] í‰ê°€ ë¶ˆì¼ì¹˜ ì¼€ì´ìŠ¤ ë¶„ì„
- [ ] ë¶„ì„ ë³´ê³ ì„œ ìƒì„±
- [ ] íŠœë‹ ë°©í–¥ ë„ì¶œ
- [ ] (ì„ íƒ) íŠœë‹ í›„ ì¬ì‹¤í—˜

**ìµœì¢… ì‚°ì¶œë¬¼**:
- `data/results/comparison.csv` - ì „ì²´ ë¹„êµ ê²°ê³¼
- `data/results/report.md` - ë¶„ì„ ë³´ê³ ì„œ
- íŠœë‹ ê¶Œì¥ì‚¬í•­ ë° ë‹¤ìŒ ë‹¨ê³„ ê³„íš
