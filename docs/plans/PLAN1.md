# PLAN 1: ê¸°ì¡´ RAG ë¡œì§ í…ŒìŠ¤íŠ¸ í™˜ê²½ êµ¬ì¶•

## 1.1 í”„ë¡œì íŠ¸ ì´ˆê¸° ì„¤ì •

### Python í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„±
```
strands-playground/
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ src/
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ tests/
â”œâ”€â”€ data/
â””â”€â”€ scripts/
```
- `pyproject.toml`ë¡œ ì˜ì¡´ì„± ê´€ë¦¬ (poetry ë˜ëŠ” pip)
- src ë ˆì´ì•„ì›ƒìœ¼ë¡œ import ì¶©ëŒ ë°©ì§€

### ì˜ì¡´ì„± ì„¤ì •
```toml
[project]
dependencies = [
    "opensearch-py",      # OpenSearch ì—°ê²°
    "litellm",            # LLM í†µí•© (Vertex AI Claude)
    "python-dotenv",      # í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
    "pydantic",           # ë°ì´í„° ê²€ì¦
    "pandas",             # ê²°ê³¼ íŒŒì¼ ì²˜ë¦¬
]
```

### í™˜ê²½ë³€ìˆ˜ ì„¤ì •
```env
# .env.example
OPENSEARCH_HOST=https://your-opensearch-endpoint
OPENSEARCH_USERNAME=admin
OPENSEARCH_PASSWORD=xxx
OPENSEARCH_INDEX=your-index

GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json
VERTEX_PROJECT=your-gcp-project
VERTEX_LOCATION=us-east5
```

---

## 1.2 ê¸°ì¡´ RAG ë¡œì§ ì¬êµ¬í˜„

### ê¸°ì¡´ ë¡œì§ ì½”ë“œ ë¶„ì„
- ì‚¬ìš©ìê°€ ê¸°ì¡´ ì½”ë“œ ì œê³µí•˜ë©´ ë¶„ì„
- í•µì‹¬ ë¡œì§ íŒŒì•…: ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±, í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿, í›„ì²˜ë¦¬ ë“±

### OpenSearch ì—°ê²° ëª¨ë“ˆ
```python
# src/opensearch_client.py
class OpenSearchClient:
    def __init__(self, host, username, password):
        self.client = OpenSearch(...)

    def search(self, query: str, index: str, k: int = 5) -> list[Document]:
        # ë²¡í„° ê²€ìƒ‰ ë˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        pass
```

### ê²€ìƒ‰(retrieval) ë¡œì§
- ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± (ê¸°ì¡´ ëª¨ë¸ ì‚¬ìš©)
- k-NN ê²€ìƒ‰ ë˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
- ê²°ê³¼ ë¬¸ì„œ íŒŒì‹± ë° ë°˜í™˜

### LLM í˜¸ì¶œ ë¡œì§
```python
# src/llm_client.py
from litellm import completion

def call_llm(prompt: str, context: str) -> str:
    response = completion(
        model="vertex_ai/claude-3-sonnet",
        messages=[{"role": "user", "content": f"{context}\n\n{prompt}"}],
        vertex_project=os.getenv("VERTEX_PROJECT"),
        vertex_location=os.getenv("VERTEX_LOCATION"),
    )
    return response.choices[0].message.content
```

### ê¸°ë³¸ RAG íŒŒì´í”„ë¼ì¸
```python
# src/rag/basic.py
class BasicRAG:
    def __init__(self, search_client, llm_client):
        self.search = search_client
        self.llm = llm_client

    def query(self, question: str) -> RAGResult:
        # 1. ê²€ìƒ‰
        docs = self.search.search(question, k=5)
        # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = self._build_context(docs)
        # 3. LLM í˜¸ì¶œ
        answer = self.llm.call(question, context)
        # 4. ê²°ê³¼ ë°˜í™˜
        return RAGResult(question, answer, docs, metadata)
```

---

## 1.3 ë¬¸ì„œ ë° ì§ˆë¬¸ì…‹ ì¤€ë¹„

### í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì—…ë¡œë“œ
- ì‚¬ìš©ìê°€ OpenSearchì— ì§ì ‘ ì—…ë¡œë“œ
- ë˜ëŠ” ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ì œê³µ (í•„ìš”ì‹œ)

### ë¬¸ì„œ í…ìŠ¤íŠ¸ íŒŒì¼ ìˆ˜ì‹ 
- ì‚¬ìš©ìê°€ PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œí•˜ì—¬ ì „ë‹¬
- í˜•ì‹: ë‹¨ì¼ í…ìŠ¤íŠ¸ íŒŒì¼ ë˜ëŠ” ë¬¸ì„œë³„ ë¶„ë¦¬

### ì§ˆë¬¸ì…‹ ìƒì„±
```python
# Claudeê°€ í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ ìƒì„±
ì§ˆë¬¸ ìœ í˜• ë¶„í¬ (30-50ê°œ):
â”œâ”€â”€ ë‹¨ìˆœ ì‚¬ì‹¤ ì§ˆë¬¸ (40%): "XëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
â”œâ”€â”€ ë¹„êµ/ë¶„ì„ ì§ˆë¬¸ (20%): "Aì™€ Bì˜ ì°¨ì´ì ì€?"
â”œâ”€â”€ ë‹¤ë‹¨ê³„ ì¶”ë¡  (20%): "X ìƒí™©ì—ì„œ Yë¥¼ í•˜ë ¤ë©´?"
â”œâ”€â”€ ì—£ì§€ ì¼€ì´ìŠ¤ (15%): ëª¨í˜¸í•˜ê±°ë‚˜ ë¬¸ì„œì— ì—†ëŠ” ì§ˆë¬¸
â””â”€â”€ ë³µí•© ì§ˆë¬¸ (5%): ì—¬ëŸ¬ ì£¼ì œë¥¼ ë¬»ëŠ” ì§ˆë¬¸
```

### ì§ˆë¬¸ì…‹ ê²€ìˆ˜ ë° í™•ì •
- ìƒì„±ëœ ì§ˆë¬¸ì…‹ì„ ì‚¬ìš©ìê°€ ê²€í† 
- ë¶€ì ì ˆí•œ ì§ˆë¬¸ ì œê±°/ìˆ˜ì •
- ìµœì¢… ì§ˆë¬¸ì…‹ JSON ì €ì¥

```json
// data/questions.json
[
  {"id": 1, "question": "...", "category": "fact", "difficulty": "easy"},
  {"id": 2, "question": "...", "category": "reasoning", "difficulty": "medium"},
  ...
]
```

---

## 1.4 í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° ê²°ê³¼ ì¶œë ¥

### í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
```python
# scripts/run_basic.py
def main():
    rag = BasicRAG(...)
    questions = load_questions("data/questions.json")
    results = []

    for q in questions:
        start = time.time()
        result = rag.query(q["question"])
        elapsed = time.time() - start

        results.append({
            "id": q["id"],
            "question": q["question"],
            "answer": result.answer,
            "sources": [d.id for d in result.docs],
            "latency_ms": elapsed * 1000,
            "tokens_used": result.metadata.tokens,
        })

    save_results(results, "data/results/basic_results.json")
```

### ê²°ê³¼ íŒŒì¼ í¬ë§·
```json
// data/results/basic_results.json
{
  "run_id": "basic_20241230_143000",
  "config": {"model": "claude-3-sonnet", "k": 5},
  "results": [
    {
      "id": 1,
      "question": "...",
      "answer": "...",
      "sources": ["doc_1", "doc_3"],
      "latency_ms": 1234,
      "tokens_used": 567
    },
    ...
  ],
  "summary": {
    "total_questions": 50,
    "avg_latency_ms": 1500,
    "total_tokens": 28000
  }
}
```

### ì„±ëŠ¥ ì§€í‘œ ìˆ˜ì§‘
- ì‘ë‹µ ì‹œê°„ (latency)
- í† í° ì‚¬ìš©ëŸ‰ (input/output)
- ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜
- ì—ëŸ¬ ë°œìƒ ê±´ìˆ˜

---

## ğŸ¯ Phase 1 ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„± ì™„ë£Œ
- [ ] OpenSearch ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ
- [ ] LLM í˜¸ì¶œ í…ŒìŠ¤íŠ¸ ì„±ê³µ
- [ ] ê¸°ë³¸ RAG íŒŒì´í”„ë¼ì¸ ë™ì‘ í™•ì¸
- [ ] ì§ˆë¬¸ì…‹ 30ê°œ ì´ìƒ ì¤€ë¹„ ì™„ë£Œ
- [ ] ì „ì²´ ì§ˆë¬¸ì…‹ ì‹¤í–‰ ë° ê²°ê³¼ íŒŒì¼ ìƒì„±

**ë‹¤ìŒ ë‹¨ê³„**: PLAN 2 - Strands Agent ëª¨ë“œ ì¶”ê°€
