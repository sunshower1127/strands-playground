# 향후 임베딩 및 인덱싱 개선

> 현재 프로젝트 범위 외. 추후 인덱싱 파이프라인 개선 시 검토.
>
> **최종 업데이트: 2025-12-31**

---

## 우선순위 요약

| 기술 | 복잡도 | 효과 | 도입 시점 |
|------|--------|------|----------|
| **Voyage AI Embedding** | 낮음 | 높음 | Titan 대체 필요시 |
| **Late Chunking** | 중간 | 중-상 | 대명사 참조 문제시 |
| **Semantic Chunking** | 중간 | 중-상 | 청킹 품질 이슈시 |
| **Contextual Retrieval** | 높음 | 높음 | 비용 허용시 |
| **ColBERT v2** | 높음 | 높음 | 대규모 검색시 |

---

## 1. Voyage AI Embeddings

### 2025 임베딩 모델 비교

| 모델 | vs OpenAI text-embedding-3-large | 차원 | 컨텍스트 | 비용 |
|------|----------------------------------|------|----------|------|
| **voyage-3-large** | **+9.74%** | 1024-2048 | 32K | 비슷 |
| **voyage-3.5** | **+8.26%** | 2048 | 32K | 2.2배 저렴 |
| **voyage-3.5-lite** | **+6.34%** | 2048 | 32K | 6.5배 저렴 |
| OpenAI text-embedding-3-large | 기준 | 3072 | 8K | 기준 |
| Amazon Titan | - | 1024 | 8K | 저렴 |

### Voyage AI 장점
- **32K 토큰 컨텍스트** (OpenAI 8K의 4배)
- **Matryoshka 임베딩**: 차원 조절 가능 (2048 → 256)
- **다국어 성능 우수** (한국어 포함)
- int8/binary 양자화로 **벡터DB 비용 83% 절감**

### 도입 시점
- Amazon Titan보다 높은 정확도 필요 시
- 긴 문서 임베딩이 필요할 때

### 참고 자료
- [Voyage AI - voyage-3-large 발표](https://blog.voyageai.com/2025/01/07/voyage-3-large/)
- [Voyage AI - voyage-3.5 발표](https://blog.voyageai.com/2025/05/20/voyage-3-5/)
- [Best Embedding Models 2025](https://elephas.app/blog/best-embedding-models)

---

## 2. Late Chunking (후기 청킹)

### 개요
청킹 후 임베딩이 아닌, 임베딩 후 청킹으로 문맥 보존.

```
[기존 방식]
문서 ──► 청크 분할 ──► 각 청크 개별 임베딩 (문맥 손실!)

[Late Chunking]
문서 ──► 전체 토큰 임베딩 ──► 토큰 임베딩을 청크로 분할 ──► Mean Pooling
```

### 왜 효과적인가?
- "그는", "이 회사" 같은 대명사 참조 문맥 보존
- 전체 문서의 attention 정보가 각 청크에 반영됨

### 성능
- 대명사 참조 문서에서 **10-12% 검색 정확도 향상**
- 추가 학습 없이 적용 가능
- Contextual Retrieval 대비 **비용 효율적**

### vs Contextual Retrieval
| 방식 | 비용 | 정확도 | 구현 복잡도 |
|------|------|--------|------------|
| Late Chunking | 낮음 (임베딩만) | 중-상 | 중간 |
| Contextual Retrieval | 높음 (LLM 호출) | 상 | 높음 |

### 제약 사항
- Long-context 임베딩 모델 필요 (jina-embeddings-v2 등)
- 인덱싱 시점에 적용 (쿼리 시점 아님)

### 참고 자료
- [Jina AI - Late Chunking](https://jina.ai/news/late-chunking-in-long-context-embedding-models/)
- [Weaviate - Late Chunking](https://weaviate.io/blog/late-chunking)
- [Late Chunking Paper (arXiv)](https://arxiv.org/abs/2409.04701)
- [GitHub - jina-ai/late-chunking](https://github.com/jina-ai/late-chunking)

---

## 3. Semantic Chunking (의미 기반 청킹)

### 개요
고정 크기가 아닌 의미 단위로 문서 분할.

```
[Fixed-size Chunking]
문서 ──► 500토큰씩 자르기 (문장 중간에 끊길 수 있음)

[Semantic Chunking]
문서 ──► 문장 임베딩 ──► 유사도 급변 지점에서 분할
```

### 성능 (2025 연구)
- 고정 크기 대비 **15-30% 검색 정확도 향상**
- [Max-Min Semantic Chunking](https://link.springer.com/article/10.1007/s10791-025-09638-7): 의미적 일관성 보존

### 단점
- 청킹 시 임베딩 비용 발생
- 구현 복잡도 증가

### 현실적 대안
```python
# RecursiveCharacterTextSplitter가 좋은 기본값
# 400-512 토큰, 10-20% 오버랩 → 85-90% recall
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=["\n\n", "\n", ".", " "]
)
```

### 참고 자료
- [Firecrawl - Best Chunking Strategies 2025](https://www.firecrawl.dev/blog/best-chunking-strategies-rag-2025)
- [Weaviate - Chunking Strategies](https://weaviate.io/blog/chunking-strategies-for-rag)

---

## 4. Anthropic Contextual Retrieval

### 개요
각 청크에 LLM으로 문맥 정보를 추가하는 Anthropic의 방식.

```
원본 청크: "그는 2024년 CEO가 되었다"
      ↓ LLM 문맥 추가
보강된 청크: "[이 문서는 삼성전자 이재용 회장에 대한 것입니다] 그는 2024년 CEO가 되었다"
```

### 성능 (Anthropic 공식)
- 검색 실패율 **49% 감소**
- Reranking 결합 시 **67% 감소**

### 비용 최적화: Prompt Caching
```
일반 방식: 청크마다 전체 문서 전달 → 비용 폭발

Prompt Caching 활용:
1. 전체 문서를 캐시에 한 번 로드
2. 각 청크 처리 시 캐시된 문서 참조
→ 비용 90% 절감, 레이턴시 85% 감소 (11.5s → 2.4s)
```

**예상 비용**: 청크당 약 $1.02/M 토큰 (800토큰 청크, 8K 문서 기준)

### 도입 시점
- 검색 품질이 매우 중요할 때
- 토큰 비용을 감당할 수 있을 때
- 대명사/참조가 많은 문서

### 참고 자료
- [Anthropic - Contextual Retrieval 발표](https://www.anthropic.com/news/contextual-retrieval)
- [Anthropic Engineering - Contextual Retrieval](https://www.anthropic.com/engineering/contextual-retrieval)
- [Anthropic - Prompt Caching](https://www.anthropic.com/news/prompt-caching)

---

## 5. ColBERT v2 / Late Interaction Models

### 개요
Cross-Encoder 수준 정확도 + Bi-Encoder 수준 속도를 제공하는 모델.

```
[Bi-Encoder]
Query ──► 임베딩 ──┐
                  ├──► 코사인 유사도 (빠름, 덜 정확)
Doc ──► 임베딩 ───┘

[Cross-Encoder]
(Query, Doc) ──► 함께 인코딩 ──► 점수 (느림, 정확)

[ColBERT - Late Interaction]
Query ──► 토큰별 임베딩 ──┐
                         ├──► MaxSim (빠름 + 정확)
Doc ──► 토큰별 임베딩 ────┘
```

### ColBERTv2 특징
- 공간 효율: 기존 대비 **6-10배 절감** (Residual Compression)
- [PLAID Engine](https://dl.acm.org/doi/10.1145/3511808.3557325): GPU에서 7배, CPU에서 45배 빠름
- 140M 패시지에서도 수십~수백 ms 레이턴시

### Jina-ColBERT-v2 (2024)
- **다국어 지원** 포함
- ColBERTv2 대비 개선된 학습 파이프라인

### 도입 시점
- 대규모 검색 + 높은 정확도가 모두 필요할 때
- Cross-Encoder가 너무 느릴 때

### 참고 자료
- [ColBERTv2 Paper](https://arxiv.org/abs/2112.01488)
- [Jina-ColBERT-v2 Paper](https://arxiv.org/abs/2408.16672)
- [Weaviate - Late Interaction Overview](https://weaviate.io/blog/late-interaction-overview)
- [Stanford ColBERT GitHub](https://github.com/stanford-futuredata/ColBERT)

---

## 참고 문헌

### Embeddings
1. [Voyage AI - voyage-3-large](https://blog.voyageai.com/2025/01/07/voyage-3-large/)
2. [Voyage AI - voyage-3.5](https://blog.voyageai.com/2025/05/20/voyage-3-5/)
3. [Best Embedding Models 2025](https://elephas.app/blog/best-embedding-models)

### Chunking
4. [Best Chunking Strategies for RAG 2025](https://www.firecrawl.dev/blog/best-chunking-strategies-rag-2025)
5. [Weaviate - Chunking Strategies](https://weaviate.io/blog/chunking-strategies-for-rag)
6. [Jina AI - Late Chunking](https://jina.ai/news/late-chunking-in-long-context-embedding-models/)

### Contextual Retrieval
7. [Anthropic - Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval)
8. [Anthropic - Prompt Caching](https://www.anthropic.com/news/prompt-caching)

### Late Interaction
9. [ColBERTv2 Paper](https://arxiv.org/abs/2112.01488)
10. [Jina-ColBERT-v2 Paper](https://arxiv.org/abs/2408.16672)
11. [Weaviate - Late Interaction Overview](https://weaviate.io/blog/late-interaction-overview)
