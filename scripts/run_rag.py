"""RAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ëŸ¬ë„ˆ

ì§ˆë¬¸ì…‹ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.

Usage:
    # ê¸°ë³¸ ì‹¤í–‰ (minimal pipeline)
    uv run python scripts/run_rag.py

    # íŒŒì´í”„ë¼ì¸ ì„ íƒ
    uv run python scripts/run_rag.py --pipeline minimal
    uv run python scripts/run_rag.py --pipeline standard
    uv run python scripts/run_rag.py --pipeline full

    # íŠ¹ì • ì§ˆë¬¸ë§Œ ì‹¤í–‰
    uv run python scripts/run_rag.py --questions 1,2,3

    # ë ˆë²¨ í•„í„°
    uv run python scripts/run_rag.py --level 1

    # ì„¤ì •ë§Œ ì¶œë ¥
    uv run python scripts/run_rag.py --dry-run
"""

import argparse
import json
import sys
from collections import defaultdict
from datetime import datetime
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from tqdm import tqdm

from src.rag import (
    create_full_pipeline,
    create_minimal_pipeline,
    create_standard_pipeline,
)

# =============================================================================
# ê²½ë¡œ ì„¤ì •
# =============================================================================

QUESTIONS_PATH = PROJECT_ROOT / "data" / "questions" / "question_set.json"
RESULTS_DIR = PROJECT_ROOT / "data" / "results"


# =============================================================================
# ì§ˆë¬¸ì…‹ ë¡œë“œ
# =============================================================================


def load_questions(path: Path = QUESTIONS_PATH) -> list[dict]:
    """ì§ˆë¬¸ì…‹ ë¡œë“œ"""
    with open(path, encoding="utf-8") as f:
        data = json.load(f)
    return data["questions"]


def filter_questions(
    questions: list[dict],
    question_ids: list[int] | None = None,
    level: int | None = None,
) -> list[dict]:
    """ì§ˆë¬¸ í•„í„°ë§"""
    result = questions

    if question_ids:
        result = [q for q in result if q["id"] in question_ids]

    if level:
        result = [q for q in result if q["level"] == level]

    return result


# =============================================================================
# íŒŒì´í”„ë¼ì¸ ì„¤ì •
# =============================================================================

PIPELINE_FACTORIES = {
    "minimal": create_minimal_pipeline,
    "standard": create_standard_pipeline,
    "full": create_full_pipeline,
}


def get_pipeline_config(pipeline_name: str) -> dict:
    """íŒŒì´í”„ë¼ì¸ ì„¤ì • ì •ë³´ ë°˜í™˜ (ê²°ê³¼ ì €ì¥ìš©)"""
    configs = {
        "minimal": {
            "name": "minimal",
            "preprocessor": None,
            "query_enhancer": None,
            "query_builder": "KNNQueryBuilder",
            "result_filter": None,
            "chunk_expander": None,
            "context_builder": "SimpleContextBuilder",
            "prompt_template": "SimplePromptTemplate",
            "search_size": 5,
            "search_pipeline": None,
        },
        "standard": {
            "name": "standard",
            "preprocessor": None,
            "query_enhancer": None,
            "query_builder": "HybridQueryBuilder",
            "result_filter": "TopKFilter(k=5)",
            "chunk_expander": None,
            "context_builder": "RankedContextBuilder(reorder=True)",
            "prompt_template": "StrictPromptTemplate",
            "search_size": 20,
            "search_pipeline": "hybrid-rrf",
        },
        "full": {
            "name": "full",
            "preprocessor": "KoreanPreprocessor",
            "query_enhancer": None,
            "query_builder": "HybridQueryBuilder",
            "result_filter": "CompositeFilter([TopKFilter(k=20), RerankerFilter(top_k=5)])",
            "chunk_expander": "NeighborChunkExpander(window=5)",
            "context_builder": "RankedContextBuilder(reorder=True)",
            "prompt_template": "StrictPromptTemplate",
            "search_size": 50,
            "search_pipeline": "hybrid-rrf",
        },
    }
    return configs.get(pipeline_name, configs["minimal"])


# =============================================================================
# ê²°ê³¼ ì €ì¥
# =============================================================================


def save_results(
    results: list[dict],
    config: dict,
    run_id: str,
    output_dir: Path = RESULTS_DIR,
) -> Path:
    """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    output_dir.mkdir(parents=True, exist_ok=True)

    # í†µê³„ ê³„ì‚°
    summary = calculate_summary(results)

    # ê²°ê³¼ íŒŒì¼ êµ¬ì„±
    output = {
        "run_id": run_id,
        "config": config,
        "results": results,
        "summary": summary,
    }

    # ì €ì¥
    filename = f"{run_id}_{config['name']}.json"
    output_path = output_dir / filename

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    return output_path


def calculate_summary(results: list[dict]) -> dict:
    """ê²°ê³¼ í†µê³„ ê³„ì‚°"""
    if not results:
        return {}

    total = len(results)
    latencies = [r["latency_ms"] for r in results]
    input_tokens = sum(r["input_tokens"] for r in results)
    output_tokens = sum(r["output_tokens"] for r in results)

    # ë ˆë²¨ë³„ í†µê³„
    by_level = defaultdict(lambda: {"count": 0, "latencies": []})
    for r in results:
        level = r["level"]
        by_level[level]["count"] += 1
        by_level[level]["latencies"].append(r["latency_ms"])

    level_stats = {}
    for level, data in sorted(by_level.items()):
        level_stats[str(level)] = {
            "count": data["count"],
            "avg_latency_ms": round(sum(data["latencies"]) / len(data["latencies"]), 1),
        }

    return {
        "total_questions": total,
        "avg_latency_ms": round(sum(latencies) / total, 1),
        "min_latency_ms": round(min(latencies), 1),
        "max_latency_ms": round(max(latencies), 1),
        "total_input_tokens": input_tokens,
        "total_output_tokens": output_tokens,
        "total_tokens": input_tokens + output_tokens,
        "by_level": level_stats,
    }


# =============================================================================
# ì¶œë ¥
# =============================================================================


def print_summary(results: list[dict], config: dict) -> None:
    """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    summary = calculate_summary(results)

    print("\n" + "=" * 60)
    print(f"ğŸ“Š ì‹¤í–‰ ê²°ê³¼ ìš”ì•½ ({config['name']} pipeline)")
    print("=" * 60)

    print(f"\nì´ ì§ˆë¬¸ ìˆ˜: {summary['total_questions']}")
    print(f"í‰ê·  ë ˆì´í„´ì‹œ: {summary['avg_latency_ms']:.1f}ms")
    print(f"ìµœì†Œ/ìµœëŒ€: {summary['min_latency_ms']:.1f}ms / {summary['max_latency_ms']:.1f}ms")
    print(f"\nì´ í† í°: {summary['total_tokens']:,}")
    print(f"  - ì…ë ¥: {summary['total_input_tokens']:,}")
    print(f"  - ì¶œë ¥: {summary['total_output_tokens']:,}")

    print("\në ˆë²¨ë³„ í†µê³„:")
    for level, stats in summary["by_level"].items():
        print(f"  Level {level}: {stats['count']}ê°œ, í‰ê·  {stats['avg_latency_ms']:.1f}ms")

    print("=" * 60)


def print_config(config: dict) -> None:
    """ì„¤ì • ì¶œë ¥ (dry-runìš©)"""
    print("\n" + "=" * 60)
    print(f"ğŸ”§ íŒŒì´í”„ë¼ì¸ ì„¤ì •: {config['name']}")
    print("=" * 60)

    for key, value in config.items():
        if key != "name":
            print(f"  {key}: {value}")

    print("=" * 60)


# =============================================================================
# ë©”ì¸
# =============================================================================


def main():
    parser = argparse.ArgumentParser(description="RAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ëŸ¬ë„ˆ")
    parser.add_argument(
        "--pipeline",
        choices=["minimal", "standard", "full"],
        default="minimal",
        help="ì‚¬ìš©í•  íŒŒì´í”„ë¼ì¸ (ê¸°ë³¸: minimal)",
    )
    parser.add_argument(
        "--questions",
        type=str,
        help="ì‹¤í–‰í•  ì§ˆë¬¸ ID (ì‰¼í‘œ êµ¬ë¶„, ì˜ˆ: 1,2,3)",
    )
    parser.add_argument(
        "--level",
        type=int,
        choices=[1, 2, 3, 4],
        help="ì‹¤í–‰í•  ë ˆë²¨ í•„í„°",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ì„¤ì •ë§Œ ì¶œë ¥í•˜ê³  ì¢…ë£Œ",
    )
    parser.add_argument(
        "--project-id",
        type=int,
        default=334,
        help="í”„ë¡œì íŠ¸ ID (ê¸°ë³¸: 334)",
    )

    args = parser.parse_args()

    # ì„¤ì • ë¡œë“œ
    config = get_pipeline_config(args.pipeline)

    # dry-run
    if args.dry_run:
        print_config(config)
        questions = load_questions()
        question_ids = [int(x) for x in args.questions.split(",")] if args.questions else None
        filtered = filter_questions(questions, question_ids, args.level)
        print(f"\nì‹¤í–‰ ëŒ€ìƒ: {len(filtered)}ê°œ ì§ˆë¬¸")
        for q in filtered:
            print(f"  [{q['id']}] Level {q['level']}: {q['question'][:40]}...")
        return

    # íŒŒì´í”„ë¼ì¸ ìƒì„±
    print(f"\nğŸš€ íŒŒì´í”„ë¼ì¸ ìƒì„± ì¤‘... ({args.pipeline})")
    factory = PIPELINE_FACTORIES[args.pipeline]
    pipeline = factory(project_id=args.project_id)

    # ì§ˆë¬¸ ë¡œë“œ ë° í•„í„°
    questions = load_questions()
    question_ids = [int(x) for x in args.questions.split(",")] if args.questions else None
    questions = filter_questions(questions, question_ids, args.level)

    if not questions:
        print("âŒ ì‹¤í–‰í•  ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"ğŸ“ {len(questions)}ê°œ ì§ˆë¬¸ ì‹¤í–‰ ì˜ˆì •\n")

    # ì‹¤í–‰
    results = []
    for q in tqdm(questions, desc="ì§ˆë¬¸ ì²˜ë¦¬"):
        try:
            result = pipeline.query(q["question"])
            results.append({
                "id": q["id"],
                "level": q["level"],
                "category": q["category"],
                "question": q["question"],
                "expected_answer": q.get("expected_answer", ""),
                "key_facts": q.get("key_facts", []),
                "documents_required": q.get("documents_required", []),
                "answer": result.answer,
                "sources": [
                    {
                        "file_name": s["_source"].get("file_name", "unknown"),
                        "score": s.get("_score", 0),
                    }
                    for s in result.sources
                ],
                "input_tokens": result.input_tokens,
                "output_tokens": result.output_tokens,
                "latency_ms": round(result.latency_ms, 1),
                "model": result.model,
            })
        except Exception as e:
            print(f"\nâŒ ì§ˆë¬¸ {q['id']} ì‹¤íŒ¨: {e}")
            results.append({
                "id": q["id"],
                "level": q["level"],
                "category": q["category"],
                "question": q["question"],
                "expected_answer": q.get("expected_answer", ""),
                "key_facts": q.get("key_facts", []),
                "documents_required": q.get("documents_required", []),
                "answer": f"ERROR: {e}",
                "sources": [],
                "input_tokens": 0,
                "output_tokens": 0,
                "latency_ms": 0,
                "model": "",
                "error": str(e),
            })

    # ê²°ê³¼ ì €ì¥
    run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = save_results(results, config, run_id)
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")

    # ìš”ì•½ ì¶œë ¥
    print_summary(results, config)


if __name__ == "__main__":
    main()
